{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "from random import shuffle\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing chinese whispers algorithm\n",
    "As we have already defined the clustering algorithm in `clusterer.py` we can directly import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clusterer import draw_graph\n",
    "from clusterer import chineseWhispers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings for the faces in the dataset (LFW in this case) should be loaded. The embeddings can be computed from the script\n",
    "`embedder.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Chinese Whispers on the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"embeddings.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the embeddings are loaded we can then create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating graph: 100%|███████████████████████████████████████████████████████████▉| 13806/13807 [03:40<00:00, 62.63it/s]\n",
      "Iterations: 100%|██████████████████████████████████████████████████████████████████████| 20/20 [00:19<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "graph = draw_graph(data,0.8)\n",
    "graph = chineseWhispers(graph,20)\n",
    "# Takes about 5 minutes for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric used is the F-score.  \n",
    "F-score is defined as the harmonic mean of precision and recall.\n",
    "\n",
    "F-score = (2*precison*recall)/(precision + recall) \n",
    "(Replace with latex later)\n",
    "\n",
    "We will first calculate the precision and recall using True positives, False positives and False negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for nCr\n",
    "def nCr(n,r):\n",
    "    fact = math.factorial\n",
    "    return fact(n)/(fact(r)*fact(n-r))\n",
    "\n",
    "def partial_dict_view(dictionary,n):\n",
    "    pp.pprint({k: v for i, (k, v) in enumerate(dictionary.items()) if i < n})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly create a dictionary which maps a cluster to the number of images it contains.  \n",
    "**NOTE:** a lot of people in the LFW dataset have only one image of them, hence there can be many clusters with only one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_num_images = {}\n",
    "for node in graph.nodes:\n",
    "    if graph.nodes[node]['pseudoClass'] in cluster_to_num_images:\n",
    "        cluster_to_num_images[graph.nodes[node]['pseudoClass']] += 1\n",
    "    else:\n",
    "        cluster_to_num_images[graph.nodes[node]['pseudoClass']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 1, 3: 1, 7: 4, 8: 1, 9: 2, 12: 1, 13: 2, 14: 1, 5375: 4}\n"
     ]
    }
   ],
   "source": [
    "partial_dict_view(cluster_to_num_images,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the total positives\n",
    "(add explanation later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pw_positives = 0\n",
    "for cluster,num_images in cluster_to_num_images.items():\n",
    "    # It's a positive only if a pair can be formed\n",
    "    if num_images >= 2:\n",
    "        total_pw_positives += nCr(num_images,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269571.0\n"
     ]
    }
   ],
   "source": [
    "print(total_pw_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The subdirectory of the image in which it resides in the LFW folder is the identity of the person. \n",
    "- In this scenario the identity is the class.\n",
    "- Hence the  classes can be taken from the subdirectory in the path of the image.\n",
    "- The nodes of the networkx graph contain an attribute which has the relative path for the image.\n",
    "- For example `lfw\\\\Aaron_Eckhart\\\\Aaron_Eckhart_0001.jpg` is the image corresponding to the person named Aaron Eckhart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we create a dictionary which maps a cluster to a dictionary which describes the cluster. i.e the latter's value is a dictionary which maps each class (identity) in the cluster to the number of nodes (image embeddings) in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = list(cluster_to_num_images.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_desc = {}\n",
    "for cluster in clusters:\n",
    "    cluster_to_desc[cluster] = {}\n",
    "    for node in graph.nodes:\n",
    "        if graph.nodes[node]['pseudoClass'] == cluster:\n",
    "            \n",
    "            Class = os.path.split(graph.nodes[node]['path'])[0]\n",
    "            Class = Class[4:]\n",
    "            # find a cleaner implementation\n",
    "            if Class not in cluster_to_desc[cluster]:\n",
    "                cluster_to_desc[cluster][Class] = 1\n",
    "            else:\n",
    "                cluster_to_desc[cluster][Class] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'Aaron_Eckhart': 1},\n",
      " 2: {'Aaron_Guiel': 1},\n",
      " 3: {'Aaron_Patterson': 1},\n",
      " 7: {'Aaron_Peirsol': 4},\n",
      " 8: {'Aaron_Pena': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Detailed description of the dictionary\n",
    "partial_dict_view(cluster_to_desc,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the true positives. (give explanation later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pw_positives = 0\n",
    "for cluster in cluster_to_desc.keys():\n",
    "    for Class in cluster_to_desc[cluster].keys():\n",
    "        if cluster_to_desc[cluster][Class] >= 2:\n",
    "            true_pw_positives += nCr(cluster_to_desc[cluster][Class],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238593.0\n"
     ]
    }
   ],
   "source": [
    "print(true_pw_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positives can then be calculated from subtracting from the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30978.0\n"
     ]
    }
   ],
   "source": [
    "false_pw_positives = total_pw_positives - true_pw_positives\n",
    "print(false_pw_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a dictionary which maps a class(identity) to the number of images it contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_num_images = {}\n",
    "for Class in os.listdir(\"lfw\"):\n",
    "    num_images = 0\n",
    "    for image in os.listdir(os.path.join(\"lfw\",Class)):\n",
    "        num_images += 1\n",
    "    class_to_num_images[Class] = num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aaron_Eckhart': 1,\n",
      " 'Aaron_Guiel': 1,\n",
      " 'Aaron_Patterson': 1,\n",
      " 'Aaron_Peirsol': 4,\n",
      " 'Aaron_Pena': 1,\n",
      " 'Aaron_Sorkin': 2,\n",
      " 'Aaron_Tippin': 1,\n",
      " 'Abba_Eban': 1,\n",
      " 'Abbas_Kiarostami': 1,\n",
      " 'Abdel_Aziz_Al-Hakim': 1}\n"
     ]
    }
   ],
   "source": [
    "partial_dict_view(class_to_num_images,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the false negatives using the above dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pw_negatives = 0\n",
    "# Iterate through all classes\n",
    "for Class in class_to_num_images.keys():\n",
    "    prev_occurence = 0\n",
    "    for cluster in cluster_to_num_images.keys():\n",
    "        if Class in cluster_to_desc[cluster]:\n",
    "            # Get the number of nodes for the current class in the cluster\n",
    "            num_of_class_in_cluster = cluster_to_desc[cluster][Class]\n",
    "            \n",
    "            # Get the number of nodes for the current class not in the cluster\n",
    "            # It can be calculated by subtracting the number of times a class occurs in the cluster\n",
    "            # from the total number of times it occurs in the dataset\n",
    "            num_of_class_out_of_cluster = class_to_num_images[Class] - num_of_class_in_cluster\n",
    "            \n",
    "            # The number of pairs formed containing mismatched nodes is the number of nodes in\n",
    "            # the cluster multiplied with the number out of the cluster\n",
    "            # To account for pairs added by the previous cluster we subtract 'prev'\n",
    "            false_pw_negatives += num_of_class_in_cluster*(num_of_class_out_of_cluster-prev_occurence)\n",
    "            prev_occurence += num_of_class_in_cluster\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894\n"
     ]
    }
   ],
   "source": [
    "print(false_pw_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally with all these values we can calculate the F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.89\n",
      "Recall: 0.99\n",
      "F measure: 0.94\n"
     ]
    }
   ],
   "source": [
    "precision = true_pw_positives / (true_pw_positives + false_pw_positives)\n",
    "\n",
    "recall = true_pw_positives / (true_pw_positives + false_pw_negatives)\n",
    "\n",
    "f_measure = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F measure: {:.2f}\".format(f_measure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-cpu] *",
   "language": "python",
   "name": "conda-env-tensorflow-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
